{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Churn Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "<ul>\n",
    "<li>[Part 1: Data Exploration]\n",
    "<li>[Part 2: Feature Preprocessing]\n",
    "<li>[Part 3: Model Training and Results Evaluation]\n",
    "<li>[Part 4: Prediction Results]\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1:Data exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "missing_values = [\"#\",'']\n",
    "train=pd.read_csv('dati/train_set.csv',na_values=missing_values)\n",
    "test=pd.read_csv('dati/test_set.csv',na_values=missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Define a function to visulize the features with missing values, and % of total values, & datatype\n",
    "def missing_values_table(df):\n",
    "     # Total missing values\n",
    "    mis_val = df.isnull().sum()\n",
    "    # Percentage of missing values\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "    mis_val_type = df.dtypes\n",
    "    # Make a table with the results\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent, mis_val_type], axis=1)\n",
    "        \n",
    "     # Rename the columns\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(columns = {0 : 'Missing Values', 1 : '% of Total Values', 2: 'type'})\n",
    "        \n",
    "    # Sort the table by percentage of missing descending\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[ mis_val_table_ren_columns.iloc[:,1] != 0].sort_values('% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "    # Print some summary information\n",
    "    print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\" \"There are \" + str(mis_val_table_ren_columns.shape[0]) + \" columns that have missing values.\")\n",
    "        \n",
    "    # Return the dataframe with missing information\n",
    "    return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# make general plots to examine each feature\n",
    "def plot_var(col_name, full_name, continuous,df):\n",
    "    \"\"\"\n",
    "    Visualize a variable with/without faceting on the loan status.\n",
    "    - col_name is the variable name in the dataframe\n",
    "    - full_name is the full variable name\n",
    "    - continuous is True for continuous variables\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, sharex=False, figsize=(15,3))\n",
    "    # plot1: counts distribution of the variable\n",
    "    \n",
    "    if continuous:  \n",
    "        sns.distplot(df.loc[df[col_name].notnull(), col_name], kde=False, ax=ax1)\n",
    "    else:\n",
    "        sns.countplot(df[col_name], order=sorted(df[col_name].unique()), color='#5975A4', saturation=1, ax=ax1)\n",
    "    ax1.set_xlabel(full_name)\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.set_title(full_name)\n",
    "\n",
    "          \n",
    "    # plot2: bar plot of the variable grouped by churned or not\n",
    "    if continuous:\n",
    "        sns.boxplot(x=col_name, y='target', data=df, ax=ax2)\n",
    "        ax2.set_ylabel('')\n",
    "        ax2.set_title(full_name + ' by Loan Status')\n",
    "    else:\n",
    "        Charged_Off_rates = df.groupby(col_name)['target'].value_counts(normalize=True)[:,1]\n",
    "        sns.barplot(x=Charged_Off_rates.index, y=Charged_Off_rates.values, color='#5975A4', saturation=1, ax=ax2)\n",
    "        ax2.set_ylabel('Fraction churned')\n",
    "        ax2.set_title('churned Rate by ' + full_name)\n",
    "        ax2.set_xlabel(full_name)\n",
    "    \n",
    "    # plot3: kde plot of the variable gropued by loan_status\n",
    "    if continuous:  \n",
    "        facet = sns.FacetGrid(df, hue = 'target', size=3, aspect=4)\n",
    "        facet.map(sns.kdeplot, col_name, shade=True)\n",
    "        #facet.set(xlim=(df[col_name].min(), df[col_name].max()))\n",
    "        facet.add_legend()  \n",
    "    else:\n",
    "        fig = plt.figure(figsize=(12,3))\n",
    "        sns.countplot(x=col_name, hue='target', data=df, order=sorted(df[col_name].unique()) )\n",
    "     \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate two dataset\n",
    "conc=[train,test]\n",
    "concate=pd.concat(conc,keys=['train','test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    8772\n",
       "1.0    1228\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concate.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your selected dataframe has 297 columns.\n",
      "There are 112 columns that have missing values.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>% of Total Values</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feature_38</th>\n",
       "      <td>16975</td>\n",
       "      <td>84.9</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_39</th>\n",
       "      <td>16975</td>\n",
       "      <td>84.9</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_36</th>\n",
       "      <td>16975</td>\n",
       "      <td>84.9</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_37</th>\n",
       "      <td>16975</td>\n",
       "      <td>84.9</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_245</th>\n",
       "      <td>10189</td>\n",
       "      <td>50.9</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Missing Values  % of Total Values     type\n",
       "feature_38            16975               84.9   object\n",
       "feature_39            16975               84.9   object\n",
       "feature_36            16975               84.9   object\n",
       "feature_37            16975               84.9   object\n",
       "feature_245           10189               50.9  float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing=missing_values_table(concate)\n",
    "missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean_zero=(train.groupby('target').mean()==0)\n",
    "VARIABILI_O=Mean_zero.columns[Mean_zero.sum()==0]\n",
    "VARIABILI_1=Mean_zero.columns[Mean_zero.sum()==1]\n",
    "VARIABILI_2=Mean_zero.columns[Mean_zero.sum()==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['feature_27', 'feature_43', 'feature_44', 'feature_45', 'feature_48',\n",
       "       'feature_64', 'feature_65', 'feature_154', 'feature_155', 'feature_156',\n",
       "       'feature_157', 'feature_158', 'feature_159', 'feature_160',\n",
       "       'feature_161', 'feature_162', 'feature_164', 'feature_165',\n",
       "       'feature_166', 'feature_172', 'feature_173', 'feature_174',\n",
       "       'feature_175', 'feature_176', 'feature_177', 'feature_178',\n",
       "       'feature_179', 'feature_180', 'feature_182', 'feature_183',\n",
       "       'feature_184', 'feature_208', 'feature_209', 'feature_210',\n",
       "       'feature_211', 'feature_212', 'feature_213', 'feature_214',\n",
       "       'feature_215', 'feature_216', 'feature_218', 'feature_219',\n",
       "       'feature_220', 'feature_226', 'feature_227', 'feature_228',\n",
       "       'feature_229', 'feature_230', 'feature_231', 'feature_232',\n",
       "       'feature_233', 'feature_234', 'feature_236', 'feature_237',\n",
       "       'feature_238'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VARIABILI_O\n",
    "# VARIABILI_1\n",
    "VARIABILI_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "      <th>feature_25</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>feature_29</th>\n",
       "      <th>feature_30</th>\n",
       "      <th>feature_31</th>\n",
       "      <th>feature_32</th>\n",
       "      <th>feature_33</th>\n",
       "      <th>feature_34</th>\n",
       "      <th>feature_35</th>\n",
       "      <th>feature_40</th>\n",
       "      <th>feature_41</th>\n",
       "      <th>feature_42</th>\n",
       "      <th>feature_43</th>\n",
       "      <th>feature_44</th>\n",
       "      <th>feature_45</th>\n",
       "      <th>feature_46</th>\n",
       "      <th>feature_47</th>\n",
       "      <th>feature_48</th>\n",
       "      <th>feature_49</th>\n",
       "      <th>feature_50</th>\n",
       "      <th>feature_51</th>\n",
       "      <th>feature_52</th>\n",
       "      <th>feature_53</th>\n",
       "      <th>feature_54</th>\n",
       "      <th>feature_55</th>\n",
       "      <th>feature_56</th>\n",
       "      <th>feature_57</th>\n",
       "      <th>feature_58</th>\n",
       "      <th>feature_59</th>\n",
       "      <th>feature_60</th>\n",
       "      <th>feature_61</th>\n",
       "      <th>feature_62</th>\n",
       "      <th>feature_63</th>\n",
       "      <th>feature_64</th>\n",
       "      <th>feature_65</th>\n",
       "      <th>feature_66</th>\n",
       "      <th>feature_67</th>\n",
       "      <th>feature_68</th>\n",
       "      <th>feature_69</th>\n",
       "      <th>feature_70</th>\n",
       "      <th>feature_71</th>\n",
       "      <th>feature_72</th>\n",
       "      <th>feature_73</th>\n",
       "      <th>feature_74</th>\n",
       "      <th>feature_75</th>\n",
       "      <th>feature_76</th>\n",
       "      <th>feature_77</th>\n",
       "      <th>feature_78</th>\n",
       "      <th>feature_79</th>\n",
       "      <th>feature_80</th>\n",
       "      <th>feature_81</th>\n",
       "      <th>feature_82</th>\n",
       "      <th>feature_83</th>\n",
       "      <th>feature_84</th>\n",
       "      <th>feature_85</th>\n",
       "      <th>feature_86</th>\n",
       "      <th>feature_87</th>\n",
       "      <th>feature_88</th>\n",
       "      <th>feature_89</th>\n",
       "      <th>feature_90</th>\n",
       "      <th>feature_91</th>\n",
       "      <th>feature_92</th>\n",
       "      <th>feature_93</th>\n",
       "      <th>feature_94</th>\n",
       "      <th>feature_95</th>\n",
       "      <th>feature_96</th>\n",
       "      <th>feature_97</th>\n",
       "      <th>feature_98</th>\n",
       "      <th>feature_99</th>\n",
       "      <th>feature_100</th>\n",
       "      <th>feature_101</th>\n",
       "      <th>feature_102</th>\n",
       "      <th>feature_103</th>\n",
       "      <th>feature_104</th>\n",
       "      <th>feature_105</th>\n",
       "      <th>feature_106</th>\n",
       "      <th>feature_107</th>\n",
       "      <th>feature_108</th>\n",
       "      <th>feature_109</th>\n",
       "      <th>feature_110</th>\n",
       "      <th>feature_111</th>\n",
       "      <th>feature_112</th>\n",
       "      <th>feature_113</th>\n",
       "      <th>feature_114</th>\n",
       "      <th>feature_115</th>\n",
       "      <th>feature_116</th>\n",
       "      <th>feature_117</th>\n",
       "      <th>feature_118</th>\n",
       "      <th>feature_119</th>\n",
       "      <th>feature_120</th>\n",
       "      <th>feature_121</th>\n",
       "      <th>feature_122</th>\n",
       "      <th>feature_123</th>\n",
       "      <th>feature_124</th>\n",
       "      <th>feature_125</th>\n",
       "      <th>feature_126</th>\n",
       "      <th>feature_127</th>\n",
       "      <th>feature_128</th>\n",
       "      <th>feature_129</th>\n",
       "      <th>feature_130</th>\n",
       "      <th>feature_131</th>\n",
       "      <th>feature_132</th>\n",
       "      <th>feature_133</th>\n",
       "      <th>feature_134</th>\n",
       "      <th>feature_135</th>\n",
       "      <th>feature_136</th>\n",
       "      <th>feature_137</th>\n",
       "      <th>feature_138</th>\n",
       "      <th>feature_139</th>\n",
       "      <th>feature_140</th>\n",
       "      <th>feature_141</th>\n",
       "      <th>feature_142</th>\n",
       "      <th>feature_143</th>\n",
       "      <th>feature_144</th>\n",
       "      <th>feature_145</th>\n",
       "      <th>feature_146</th>\n",
       "      <th>feature_147</th>\n",
       "      <th>feature_148</th>\n",
       "      <th>feature_149</th>\n",
       "      <th>feature_150</th>\n",
       "      <th>feature_151</th>\n",
       "      <th>feature_152</th>\n",
       "      <th>feature_153</th>\n",
       "      <th>feature_154</th>\n",
       "      <th>feature_155</th>\n",
       "      <th>feature_156</th>\n",
       "      <th>feature_157</th>\n",
       "      <th>feature_158</th>\n",
       "      <th>feature_159</th>\n",
       "      <th>feature_160</th>\n",
       "      <th>feature_161</th>\n",
       "      <th>feature_162</th>\n",
       "      <th>feature_163</th>\n",
       "      <th>feature_164</th>\n",
       "      <th>feature_165</th>\n",
       "      <th>feature_166</th>\n",
       "      <th>feature_167</th>\n",
       "      <th>feature_168</th>\n",
       "      <th>feature_169</th>\n",
       "      <th>feature_170</th>\n",
       "      <th>feature_171</th>\n",
       "      <th>feature_172</th>\n",
       "      <th>feature_173</th>\n",
       "      <th>feature_174</th>\n",
       "      <th>feature_175</th>\n",
       "      <th>feature_176</th>\n",
       "      <th>feature_177</th>\n",
       "      <th>feature_178</th>\n",
       "      <th>feature_179</th>\n",
       "      <th>feature_180</th>\n",
       "      <th>feature_181</th>\n",
       "      <th>feature_182</th>\n",
       "      <th>feature_183</th>\n",
       "      <th>feature_184</th>\n",
       "      <th>feature_185</th>\n",
       "      <th>feature_186</th>\n",
       "      <th>feature_187</th>\n",
       "      <th>feature_188</th>\n",
       "      <th>feature_189</th>\n",
       "      <th>feature_190</th>\n",
       "      <th>feature_191</th>\n",
       "      <th>feature_192</th>\n",
       "      <th>feature_193</th>\n",
       "      <th>feature_194</th>\n",
       "      <th>feature_195</th>\n",
       "      <th>feature_196</th>\n",
       "      <th>feature_197</th>\n",
       "      <th>feature_198</th>\n",
       "      <th>feature_199</th>\n",
       "      <th>feature_200</th>\n",
       "      <th>feature_201</th>\n",
       "      <th>feature_202</th>\n",
       "      <th>feature_203</th>\n",
       "      <th>feature_204</th>\n",
       "      <th>feature_205</th>\n",
       "      <th>feature_206</th>\n",
       "      <th>feature_207</th>\n",
       "      <th>feature_208</th>\n",
       "      <th>feature_209</th>\n",
       "      <th>feature_210</th>\n",
       "      <th>feature_211</th>\n",
       "      <th>feature_212</th>\n",
       "      <th>feature_213</th>\n",
       "      <th>feature_214</th>\n",
       "      <th>feature_215</th>\n",
       "      <th>feature_216</th>\n",
       "      <th>feature_217</th>\n",
       "      <th>feature_218</th>\n",
       "      <th>feature_219</th>\n",
       "      <th>feature_220</th>\n",
       "      <th>feature_221</th>\n",
       "      <th>feature_222</th>\n",
       "      <th>feature_223</th>\n",
       "      <th>feature_224</th>\n",
       "      <th>feature_225</th>\n",
       "      <th>feature_226</th>\n",
       "      <th>feature_227</th>\n",
       "      <th>feature_228</th>\n",
       "      <th>feature_229</th>\n",
       "      <th>feature_230</th>\n",
       "      <th>feature_231</th>\n",
       "      <th>feature_232</th>\n",
       "      <th>feature_233</th>\n",
       "      <th>feature_234</th>\n",
       "      <th>feature_235</th>\n",
       "      <th>feature_236</th>\n",
       "      <th>feature_237</th>\n",
       "      <th>feature_238</th>\n",
       "      <th>feature_239</th>\n",
       "      <th>feature_240</th>\n",
       "      <th>feature_241</th>\n",
       "      <th>feature_242</th>\n",
       "      <th>feature_243</th>\n",
       "      <th>feature_244</th>\n",
       "      <th>feature_245</th>\n",
       "      <th>feature_246</th>\n",
       "      <th>feature_247</th>\n",
       "      <th>feature_248</th>\n",
       "      <th>feature_249</th>\n",
       "      <th>feature_250</th>\n",
       "      <th>feature_251</th>\n",
       "      <th>feature_252</th>\n",
       "      <th>feature_253</th>\n",
       "      <th>feature_254</th>\n",
       "      <th>feature_255</th>\n",
       "      <th>feature_256</th>\n",
       "      <th>feature_257</th>\n",
       "      <th>feature_258</th>\n",
       "      <th>feature_259</th>\n",
       "      <th>feature_260</th>\n",
       "      <th>feature_261</th>\n",
       "      <th>feature_262</th>\n",
       "      <th>feature_263</th>\n",
       "      <th>feature_264</th>\n",
       "      <th>feature_265</th>\n",
       "      <th>feature_266</th>\n",
       "      <th>feature_267</th>\n",
       "      <th>feature_268</th>\n",
       "      <th>feature_269</th>\n",
       "      <th>feature_270</th>\n",
       "      <th>feature_271</th>\n",
       "      <th>feature_272</th>\n",
       "      <th>feature_273</th>\n",
       "      <th>feature_274</th>\n",
       "      <th>feature_275</th>\n",
       "      <th>feature_276</th>\n",
       "      <th>feature_277</th>\n",
       "      <th>feature_278</th>\n",
       "      <th>feature_279</th>\n",
       "      <th>feature_280</th>\n",
       "      <th>feature_281</th>\n",
       "      <th>feature_282</th>\n",
       "      <th>feature_283</th>\n",
       "      <th>feature_284</th>\n",
       "      <th>feature_285</th>\n",
       "      <th>feature_286</th>\n",
       "      <th>feature_287</th>\n",
       "      <th>feature_288</th>\n",
       "      <th>feature_289</th>\n",
       "      <th>feature_290</th>\n",
       "      <th>feature_291</th>\n",
       "      <th>feature_292</th>\n",
       "      <th>feature_293</th>\n",
       "      <th>feature_294</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.313978e+06</td>\n",
       "      <td>0.027012</td>\n",
       "      <td>0.260316</td>\n",
       "      <td>0.014602</td>\n",
       "      <td>0.094788</td>\n",
       "      <td>0.109534</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.637858</td>\n",
       "      <td>0.621106</td>\n",
       "      <td>0.002818</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.004180</td>\n",
       "      <td>0.331868</td>\n",
       "      <td>5.613924</td>\n",
       "      <td>0.174820</td>\n",
       "      <td>0.053739</td>\n",
       "      <td>0.165432</td>\n",
       "      <td>0.082414</td>\n",
       "      <td>0.029729</td>\n",
       "      <td>0.082160</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>0.063385</td>\n",
       "      <td>0.004389</td>\n",
       "      <td>0.002502</td>\n",
       "      <td>0.052174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012586</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>2.143252e-04</td>\n",
       "      <td>1.067007e-05</td>\n",
       "      <td>0.043214</td>\n",
       "      <td>0.340364</td>\n",
       "      <td>0.024121</td>\n",
       "      <td>0.083995</td>\n",
       "      <td>0.036291</td>\n",
       "      <td>0.064482</td>\n",
       "      <td>0.00505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.083473</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>0.001258</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.981594</td>\n",
       "      <td>1.638827</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>0.009421</td>\n",
       "      <td>0.005937</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.029257</td>\n",
       "      <td>0.014381</td>\n",
       "      <td>0.014823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.082978</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1.760680e-05</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1.786572e-05</td>\n",
       "      <td>0.083346</td>\n",
       "      <td>0.086880</td>\n",
       "      <td>2.405412</td>\n",
       "      <td>0.124661</td>\n",
       "      <td>0.073570</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>0.002508</td>\n",
       "      <td>0.013576</td>\n",
       "      <td>0.007274</td>\n",
       "      <td>0.021432</td>\n",
       "      <td>0.008996</td>\n",
       "      <td>0.008517</td>\n",
       "      <td>0.025936</td>\n",
       "      <td>0.061268</td>\n",
       "      <td>0.042606</td>\n",
       "      <td>0.103047</td>\n",
       "      <td>0.005957</td>\n",
       "      <td>0.041423</td>\n",
       "      <td>0.012479</td>\n",
       "      <td>0.007831</td>\n",
       "      <td>0.027767</td>\n",
       "      <td>0.019727</td>\n",
       "      <td>0.013805</td>\n",
       "      <td>0.016763</td>\n",
       "      <td>0.086569</td>\n",
       "      <td>2.404312</td>\n",
       "      <td>0.125282</td>\n",
       "      <td>0.073884</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.002558</td>\n",
       "      <td>0.013483</td>\n",
       "      <td>0.007081</td>\n",
       "      <td>0.020617</td>\n",
       "      <td>0.009041</td>\n",
       "      <td>0.008629</td>\n",
       "      <td>0.025944</td>\n",
       "      <td>0.061514</td>\n",
       "      <td>0.042721</td>\n",
       "      <td>0.102988</td>\n",
       "      <td>0.005972</td>\n",
       "      <td>0.041717</td>\n",
       "      <td>0.012457</td>\n",
       "      <td>0.007880</td>\n",
       "      <td>0.027822</td>\n",
       "      <td>0.019857</td>\n",
       "      <td>0.013823</td>\n",
       "      <td>0.016844</td>\n",
       "      <td>0.083219</td>\n",
       "      <td>0.010714</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>0.000653</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>1.105490e-05</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>1.071840e-05</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.082758</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>2.280798e-07</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.006161</td>\n",
       "      <td>0.005177</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.008629</td>\n",
       "      <td>0.027614</td>\n",
       "      <td>0.011963</td>\n",
       "      <td>0.009078</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>0.007685</td>\n",
       "      <td>0.001926</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.009078</td>\n",
       "      <td>0.002226</td>\n",
       "      <td>0.008308</td>\n",
       "      <td>0.001926</td>\n",
       "      <td>1.081234e-06</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.007579</td>\n",
       "      <td>0.018697</td>\n",
       "      <td>0.010133</td>\n",
       "      <td>4.558395e-04</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>6.952627e-09</td>\n",
       "      <td>0.083117</td>\n",
       "      <td>0.008658</td>\n",
       "      <td>0.027407</td>\n",
       "      <td>0.011870</td>\n",
       "      <td>0.009114</td>\n",
       "      <td>0.002297</td>\n",
       "      <td>0.007762</td>\n",
       "      <td>0.001945</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>0.009114</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.008387</td>\n",
       "      <td>0.001945</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.018671</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.082759</td>\n",
       "      <td>0.083653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.484867e+06</td>\n",
       "      <td>0.012086</td>\n",
       "      <td>0.257433</td>\n",
       "      <td>0.046511</td>\n",
       "      <td>0.039902</td>\n",
       "      <td>0.060095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.585109</td>\n",
       "      <td>0.630830</td>\n",
       "      <td>0.002561</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.004196</td>\n",
       "      <td>0.310794</td>\n",
       "      <td>12.440263</td>\n",
       "      <td>0.210797</td>\n",
       "      <td>0.091118</td>\n",
       "      <td>0.144116</td>\n",
       "      <td>0.081250</td>\n",
       "      <td>0.017609</td>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.001959</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.030168</td>\n",
       "      <td>0.004239</td>\n",
       "      <td>0.001627</td>\n",
       "      <td>0.038184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010386</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>1.527898e-07</td>\n",
       "      <td>9.271504e-08</td>\n",
       "      <td>0.031538</td>\n",
       "      <td>0.313937</td>\n",
       "      <td>0.027612</td>\n",
       "      <td>0.085477</td>\n",
       "      <td>0.045647</td>\n",
       "      <td>0.051585</td>\n",
       "      <td>0.00243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086404</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>1.133669</td>\n",
       "      <td>1.722714</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.009256</td>\n",
       "      <td>0.003988</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.030833</td>\n",
       "      <td>0.042817</td>\n",
       "      <td>0.046511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.081275</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>7.506340e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>8.013460e-07</td>\n",
       "      <td>0.088710</td>\n",
       "      <td>0.076883</td>\n",
       "      <td>2.426512</td>\n",
       "      <td>0.140394</td>\n",
       "      <td>0.086449</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.002851</td>\n",
       "      <td>0.015358</td>\n",
       "      <td>0.008585</td>\n",
       "      <td>0.020112</td>\n",
       "      <td>0.012522</td>\n",
       "      <td>0.012450</td>\n",
       "      <td>0.028862</td>\n",
       "      <td>0.060713</td>\n",
       "      <td>0.046741</td>\n",
       "      <td>0.100225</td>\n",
       "      <td>0.005881</td>\n",
       "      <td>0.034999</td>\n",
       "      <td>0.014232</td>\n",
       "      <td>0.006887</td>\n",
       "      <td>0.025618</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.014530</td>\n",
       "      <td>0.017915</td>\n",
       "      <td>0.076708</td>\n",
       "      <td>2.410216</td>\n",
       "      <td>0.141284</td>\n",
       "      <td>0.086997</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>0.002889</td>\n",
       "      <td>0.015467</td>\n",
       "      <td>0.008438</td>\n",
       "      <td>0.019292</td>\n",
       "      <td>0.012627</td>\n",
       "      <td>0.012613</td>\n",
       "      <td>0.028966</td>\n",
       "      <td>0.061289</td>\n",
       "      <td>0.047317</td>\n",
       "      <td>0.098883</td>\n",
       "      <td>0.005934</td>\n",
       "      <td>0.035364</td>\n",
       "      <td>0.014226</td>\n",
       "      <td>0.006947</td>\n",
       "      <td>0.025447</td>\n",
       "      <td>0.019463</td>\n",
       "      <td>0.014511</td>\n",
       "      <td>0.018289</td>\n",
       "      <td>0.081069</td>\n",
       "      <td>0.017609</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.001409</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>6.518736e-08</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.001058</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002622</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>3.262026e-08</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001789</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086293</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>1.900617e-07</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.004812</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.012279</td>\n",
       "      <td>0.034924</td>\n",
       "      <td>0.015642</td>\n",
       "      <td>0.012908</td>\n",
       "      <td>0.003025</td>\n",
       "      <td>0.011980</td>\n",
       "      <td>0.001735</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.012908</td>\n",
       "      <td>0.002918</td>\n",
       "      <td>0.012872</td>\n",
       "      <td>0.001735</td>\n",
       "      <td>9.624620e-07</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.011752</td>\n",
       "      <td>0.027091</td>\n",
       "      <td>0.013648</td>\n",
       "      <td>2.189065e-10</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>5.982755e-04</td>\n",
       "      <td>0.080789</td>\n",
       "      <td>0.012293</td>\n",
       "      <td>0.034987</td>\n",
       "      <td>0.015517</td>\n",
       "      <td>0.012888</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>0.011955</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.012888</td>\n",
       "      <td>0.002941</td>\n",
       "      <td>0.012851</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>0.011785</td>\n",
       "      <td>0.027385</td>\n",
       "      <td>0.013675</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.083958</td>\n",
       "      <td>0.081361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               index  feature_0  feature_1  feature_2  feature_3  feature_4  \\\n",
       "target                                                                        \n",
       "0       8.313978e+06   0.027012   0.260316   0.014602   0.094788   0.109534   \n",
       "1       8.484867e+06   0.012086   0.257433   0.046511   0.039902   0.060095   \n",
       "\n",
       "        feature_5  feature_7  feature_8  feature_9  feature_10  feature_11  \\\n",
       "target                                                                       \n",
       "0        0.000456   0.637858   0.621106   0.002818    0.000010    0.004180   \n",
       "1        0.000000   0.585109   0.630830   0.002561    0.000009    0.004196   \n",
       "\n",
       "        feature_12  feature_13  feature_14  feature_15  feature_16  \\\n",
       "target                                                               \n",
       "0         0.331868    5.613924    0.174820    0.053739    0.165432   \n",
       "1         0.310794   12.440263    0.210797    0.091118    0.144116   \n",
       "\n",
       "        feature_17  feature_18  feature_19  feature_20  feature_21  \\\n",
       "target                                                               \n",
       "0         0.082414    0.029729    0.082160    0.001002    0.001282   \n",
       "1         0.081250    0.017609    0.082045    0.001608    0.001959   \n",
       "\n",
       "        feature_22  feature_23  feature_24  feature_25  feature_26  \\\n",
       "target                                                               \n",
       "0         0.001289    0.063385    0.004389    0.002502    0.052174   \n",
       "1         0.001951    0.030168    0.004239    0.001627    0.038184   \n",
       "\n",
       "        feature_27  feature_28  feature_29    feature_30    feature_31  \\\n",
       "target                                                                   \n",
       "0              0.0    0.012586    0.000953  2.143252e-04  1.067007e-05   \n",
       "1              0.0    0.010386    0.000995  1.527898e-07  9.271504e-08   \n",
       "\n",
       "        feature_32  feature_33  feature_34  feature_35  feature_40  \\\n",
       "target                                                               \n",
       "0         0.043214    0.340364    0.024121    0.083995    0.036291   \n",
       "1         0.031538    0.313937    0.027612    0.085477    0.045647   \n",
       "\n",
       "        feature_41  feature_42  feature_43  feature_44  feature_45  \\\n",
       "target                                                               \n",
       "0         0.064482     0.00505         0.0         0.0         0.0   \n",
       "1         0.051585     0.00243         0.0         0.0         0.0   \n",
       "\n",
       "        feature_46  feature_47  feature_48  feature_49  feature_50  \\\n",
       "target                                                               \n",
       "0         0.001139    0.001139         0.0    0.001139    0.001139   \n",
       "1         0.000000    0.000000         0.0    0.000000    0.000000   \n",
       "\n",
       "        feature_51  feature_52  feature_53  feature_54  feature_55  \\\n",
       "target                                                               \n",
       "0         0.083473    0.000952    0.001258    0.000724    0.981594   \n",
       "1         0.086404    0.001116    0.001468    0.000855    1.133669   \n",
       "\n",
       "        feature_56  feature_57  feature_58  feature_59  feature_60  \\\n",
       "target                                                               \n",
       "0         1.638827    0.001844    0.009421    0.005937    0.000742   \n",
       "1         1.722714    0.001786    0.009256    0.003988    0.000451   \n",
       "\n",
       "        feature_61  feature_62  feature_63  feature_64  feature_65  \\\n",
       "target                                                               \n",
       "0         0.029257    0.014381    0.014823         0.0         0.0   \n",
       "1         0.030833    0.042817    0.046511         0.0         0.0   \n",
       "\n",
       "        feature_66  feature_67  feature_68  feature_69  feature_70  \\\n",
       "target                                                               \n",
       "0         0.000645    0.000179    0.000012    0.082978    0.000008   \n",
       "1         0.000345    0.000002    0.000009    0.081275    0.000004   \n",
       "\n",
       "        feature_71  feature_72  feature_73  feature_74    feature_75  \\\n",
       "target                                                                 \n",
       "0         0.000045    0.000207    0.000041    0.000007  1.760680e-05   \n",
       "1         0.000243    0.000003    0.000015    0.000020  7.506340e-07   \n",
       "\n",
       "        feature_76  feature_77  feature_78  feature_79  feature_80  \\\n",
       "target                                                               \n",
       "0         0.000008    0.000043    0.000208    0.000041    0.000007   \n",
       "1         0.000004    0.000239    0.000003    0.000015    0.000018   \n",
       "\n",
       "          feature_81  feature_82  feature_83  feature_84  feature_85  \\\n",
       "target                                                                 \n",
       "0       1.786572e-05    0.083346    0.086880    2.405412    0.124661   \n",
       "1       8.013460e-07    0.088710    0.076883    2.426512    0.140394   \n",
       "\n",
       "        feature_86  feature_87  feature_88  feature_89  feature_90  \\\n",
       "target                                                               \n",
       "0         0.073570    0.000662    0.002508    0.013576    0.007274   \n",
       "1         0.086449    0.000876    0.002851    0.015358    0.008585   \n",
       "\n",
       "        feature_91  feature_92  feature_93  feature_94  feature_95  \\\n",
       "target                                                               \n",
       "0         0.021432    0.008996    0.008517    0.025936    0.061268   \n",
       "1         0.020112    0.012522    0.012450    0.028862    0.060713   \n",
       "\n",
       "        feature_96  feature_97  feature_98  feature_99  feature_100  \\\n",
       "target                                                                \n",
       "0         0.042606    0.103047    0.005957    0.041423     0.012479   \n",
       "1         0.046741    0.100225    0.005881    0.034999     0.014232   \n",
       "\n",
       "        feature_101  feature_102  feature_103  feature_104  feature_105  \\\n",
       "target                                                                    \n",
       "0          0.007831     0.027767     0.019727     0.013805     0.016763   \n",
       "1          0.006887     0.025618     0.019324     0.014530     0.017915   \n",
       "\n",
       "        feature_106  feature_107  feature_108  feature_109  feature_110  \\\n",
       "target                                                                    \n",
       "0          0.086569     2.404312     0.125282     0.073884     0.000655   \n",
       "1          0.076708     2.410216     0.141284     0.086997     0.000865   \n",
       "\n",
       "        feature_111  feature_112  feature_113  feature_114  feature_115  \\\n",
       "target                                                                    \n",
       "0          0.002558     0.013483     0.007081     0.020617     0.009041   \n",
       "1          0.002889     0.015467     0.008438     0.019292     0.012627   \n",
       "\n",
       "        feature_116  feature_117  feature_118  feature_119  feature_120  \\\n",
       "target                                                                    \n",
       "0          0.008629     0.025944     0.061514     0.042721     0.102988   \n",
       "1          0.012613     0.028966     0.061289     0.047317     0.098883   \n",
       "\n",
       "        feature_121  feature_122  feature_123  feature_124  feature_125  \\\n",
       "target                                                                    \n",
       "0          0.005972     0.041717     0.012457     0.007880     0.027822   \n",
       "1          0.005934     0.035364     0.014226     0.006947     0.025447   \n",
       "\n",
       "        feature_126  feature_127  feature_128  feature_129  feature_130  \\\n",
       "target                                                                    \n",
       "0          0.019857     0.013823     0.016844     0.083219     0.010714   \n",
       "1          0.019463     0.014511     0.018289     0.081069     0.017609   \n",
       "\n",
       "        feature_131  feature_132  feature_133  feature_134  feature_135  \\\n",
       "target                                                                    \n",
       "0          0.001320     0.000007     0.000007     0.000002     0.000002   \n",
       "1          0.001629     0.000007     0.000007     0.000002     0.000002   \n",
       "\n",
       "        feature_136  feature_137  feature_138  feature_139   feature_140  \\\n",
       "target                                                                     \n",
       "0          0.001301     0.000618     0.000653     0.000028  1.105490e-05   \n",
       "1          0.001409     0.000254     0.000812     0.000029  6.518736e-08   \n",
       "\n",
       "        feature_141  feature_142  feature_143  feature_144  feature_145  \\\n",
       "target                                                                    \n",
       "0          0.000020     0.000415     0.000079     0.000466     0.000249   \n",
       "1          0.000046     0.000226     0.000041     0.001058     0.000811   \n",
       "\n",
       "        feature_146  feature_147  feature_148  feature_149  feature_150  \\\n",
       "target                                                                    \n",
       "0          0.000114     0.000171     0.000093     0.000884     0.000045   \n",
       "1          0.000406     0.000000     0.000305     0.000600     0.000028   \n",
       "\n",
       "        feature_151  feature_152  feature_153  feature_154  feature_155  \\\n",
       "target                                                                    \n",
       "0          0.000045     0.000077     0.000077          0.0          0.0   \n",
       "1          0.000028     0.000046     0.000046          0.0          0.0   \n",
       "\n",
       "        feature_156  feature_157  feature_158  feature_159  feature_160  \\\n",
       "target                                                                    \n",
       "0               0.0          0.0          0.0          0.0          0.0   \n",
       "1               0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "        feature_161  feature_162  feature_163  feature_164  feature_165  \\\n",
       "target                                                                    \n",
       "0               0.0          0.0     0.000028          0.0          0.0   \n",
       "1               0.0          0.0     0.000204          0.0          0.0   \n",
       "\n",
       "        feature_166  feature_167  feature_168  feature_169  feature_170  \\\n",
       "target                                                                    \n",
       "0               0.0     0.000375     0.000027     0.000027     0.000041   \n",
       "1               0.0     0.000347     0.000044     0.000044     0.000048   \n",
       "\n",
       "        feature_171  feature_172  feature_173  feature_174  feature_175  \\\n",
       "target                                                                    \n",
       "0          0.000041          0.0          0.0          0.0          0.0   \n",
       "1          0.000048          0.0          0.0          0.0          0.0   \n",
       "\n",
       "        feature_176  feature_177  feature_178  feature_179  feature_180  \\\n",
       "target                                                                    \n",
       "0               0.0          0.0          0.0          0.0          0.0   \n",
       "1               0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "        feature_181  feature_182  feature_183  feature_184  feature_185  \\\n",
       "target                                                                    \n",
       "0          0.000028          0.0          0.0          0.0     0.001661   \n",
       "1          0.000051          0.0          0.0          0.0     0.002622   \n",
       "\n",
       "        feature_186  feature_187  feature_188  feature_189  feature_190  \\\n",
       "target                                                                    \n",
       "0          0.000013     0.000013     0.000003     0.000003     0.001277   \n",
       "1          0.000022     0.000022     0.000005     0.000005     0.001796   \n",
       "\n",
       "        feature_191  feature_192  feature_193   feature_194  feature_195  \\\n",
       "target                                                                     \n",
       "0          0.000434     0.000848     0.000021  1.071840e-05     0.000025   \n",
       "1          0.000051     0.001443     0.000019  3.262026e-08     0.000066   \n",
       "\n",
       "        feature_196  feature_197  feature_198  feature_199  feature_200  \\\n",
       "target                                                                    \n",
       "0          0.000646     0.000087     0.000578     0.000171     0.000071   \n",
       "1          0.000589     0.000050     0.001789     0.000558     0.000152   \n",
       "\n",
       "        feature_201  feature_202  feature_203  feature_204  feature_205  \\\n",
       "target                                                                    \n",
       "0          0.000057     0.000085     0.001141     0.000032     0.000032   \n",
       "1          0.000000     0.000305     0.000722     0.000020     0.000020   \n",
       "\n",
       "        feature_206  feature_207  feature_208  feature_209  feature_210  \\\n",
       "target                                                                    \n",
       "0          0.000057     0.000057          0.0          0.0          0.0   \n",
       "1          0.000030     0.000030          0.0          0.0          0.0   \n",
       "\n",
       "        feature_211  feature_212  feature_213  feature_214  feature_215  \\\n",
       "target                                                                    \n",
       "0               0.0          0.0          0.0          0.0          0.0   \n",
       "1               0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "        feature_216  feature_217  feature_218  feature_219  feature_220  \\\n",
       "target                                                                    \n",
       "0               0.0     0.000028          0.0          0.0          0.0   \n",
       "1               0.0     0.000204          0.0          0.0          0.0   \n",
       "\n",
       "        feature_221  feature_222  feature_223  feature_224  feature_225  \\\n",
       "target                                                                    \n",
       "0          0.000534     0.000015     0.000015     0.000026     0.000026   \n",
       "1          0.000402     0.000013     0.000013     0.000022     0.000022   \n",
       "\n",
       "        feature_226  feature_227  feature_228  feature_229  feature_230  \\\n",
       "target                                                                    \n",
       "0               0.0          0.0          0.0          0.0          0.0   \n",
       "1               0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "        feature_231  feature_232  feature_233  feature_234  feature_235  \\\n",
       "target                                                                    \n",
       "0               0.0          0.0          0.0          0.0     0.000000   \n",
       "1               0.0          0.0          0.0          0.0     0.000051   \n",
       "\n",
       "        feature_236  feature_237  feature_238  feature_239  feature_240  \\\n",
       "target                                                                    \n",
       "0               0.0          0.0          0.0     0.082758     0.000871   \n",
       "1               0.0          0.0          0.0     0.086293     0.000743   \n",
       "\n",
       "        feature_241  feature_242   feature_243  feature_244  feature_245  \\\n",
       "target                                                                     \n",
       "0          0.000394     0.000008  2.280798e-07     0.000007     0.006161   \n",
       "1          0.000512     0.000013  1.900617e-07     0.000009     0.004812   \n",
       "\n",
       "        feature_246  feature_247  feature_248  feature_249  feature_250  \\\n",
       "target                                                                    \n",
       "0          0.005177     0.000318     0.000079     0.000013     0.008629   \n",
       "1          0.004105     0.000172     0.000050     0.000001     0.012279   \n",
       "\n",
       "        feature_251  feature_252  feature_253  feature_254  feature_255  \\\n",
       "target                                                                    \n",
       "0          0.027614     0.011963     0.009078     0.002307     0.007685   \n",
       "1          0.034924     0.015642     0.012908     0.003025     0.011980   \n",
       "\n",
       "        feature_256  feature_257  feature_258  feature_259  feature_260  \\\n",
       "target                                                                    \n",
       "0          0.001926     0.000001     0.000677     0.009078     0.002226   \n",
       "1          0.001735     0.000001     0.000811     0.012908     0.002918   \n",
       "\n",
       "        feature_261  feature_262   feature_263  feature_264  feature_265  \\\n",
       "target                                                                     \n",
       "0          0.008308     0.001926  1.081234e-06     0.000663     0.007579   \n",
       "1          0.012872     0.001735  9.624620e-07     0.000818     0.011752   \n",
       "\n",
       "        feature_266  feature_267   feature_268  feature_269   feature_270  \\\n",
       "target                                                                      \n",
       "0          0.018697     0.010133  4.558395e-04     0.000070  6.952627e-09   \n",
       "1          0.027091     0.013648  2.189065e-10     0.000456  5.982755e-04   \n",
       "\n",
       "        feature_271  feature_272  feature_273  feature_274  feature_275  \\\n",
       "target                                                                    \n",
       "0          0.083117     0.008658     0.027407     0.011870     0.009114   \n",
       "1          0.080789     0.012293     0.034987     0.015517     0.012888   \n",
       "\n",
       "        feature_276  feature_277  feature_278  feature_279  feature_280  \\\n",
       "target                                                                    \n",
       "0          0.002297     0.007762     0.001945     0.000001     0.000709   \n",
       "1          0.003048     0.011955     0.001634     0.000001     0.000895   \n",
       "\n",
       "        feature_281  feature_282  feature_283  feature_284  feature_285  \\\n",
       "target                                                                    \n",
       "0          0.009114     0.002217     0.008387     0.001945     0.000001   \n",
       "1          0.012888     0.002941     0.012851     0.001634     0.000001   \n",
       "\n",
       "        feature_286  feature_287  feature_288  feature_289  feature_290  \\\n",
       "target                                                                    \n",
       "0          0.000692     0.007692     0.018671     0.010140     0.001772   \n",
       "1          0.000890     0.011785     0.027385     0.013675     0.001729   \n",
       "\n",
       "        feature_291  feature_292  feature_293  feature_294  \n",
       "target                                                      \n",
       "0          0.001713     0.000142     0.082759     0.083653  \n",
       "1          0.002107     0.000615     0.083958     0.081361  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Var_zero=(train.groupby('target').var())\n",
    "Var_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "concate=concate.drop(VARIABILI_2,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 242)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concate.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part 2: Feature Preprocessing\n",
    "### Near zero variance  Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=20000\n",
    "def NEAR_ZERO_VAR(df,freqcut=95/5,unique=10):\n",
    "    dfObj = pd.DataFrame(columns=['Name','freq_ratio', 'perc_unique','Nzv','lenght'])\n",
    "    colonne=df.columns\n",
    "    for i in colonne:\n",
    "        v=pd.value_counts(df[i].values, sort=True,dropna= False)\n",
    "        freq_ratio=(v.iloc[0,]/v.iloc[1,])\n",
    "        valore=len(v.index)\n",
    "        Percentunique=(len(v.index)/20000)*100\n",
    "        nzv=(freq_ratio>freqcut)*(Percentunique<unique)\n",
    "        dfObj=dfObj.append({'Name':i,'freq_ratio': freq_ratio, 'perc_unique': Percentunique,'Nzv': nzv,'lenght':valore}, ignore_index=True)\n",
    "    return (dfObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Near_zero=NEAR_ZERO_VAR(concate)\n",
    "# ?pd.DataFrame.append\n",
    "# pd.value_counts(concate.feature_19.values, sort=True,dropna= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_15',\n",
       "       'feature_18', 'feature_24', 'feature_25', 'feature_26',\n",
       "       'feature_30', 'feature_31', 'feature_32', 'feature_41',\n",
       "       'feature_42', 'feature_46', 'feature_47', 'feature_49',\n",
       "       'feature_50', 'feature_62', 'feature_63', 'feature_85',\n",
       "       'feature_86', 'feature_88', 'feature_95', 'feature_97',\n",
       "       'feature_101', 'feature_102', 'feature_103', 'feature_108',\n",
       "       'feature_109', 'feature_111', 'feature_118', 'feature_120',\n",
       "       'feature_124', 'feature_126', 'feature_130', 'feature_132',\n",
       "       'feature_133', 'feature_134', 'feature_135', 'feature_136',\n",
       "       'feature_137', 'feature_138', 'feature_139', 'feature_140',\n",
       "       'feature_141', 'feature_142', 'feature_143', 'feature_144',\n",
       "       'feature_145', 'feature_146', 'feature_147', 'feature_148',\n",
       "       'feature_149', 'feature_150', 'feature_151', 'feature_152',\n",
       "       'feature_153', 'feature_163', 'feature_167', 'feature_168',\n",
       "       'feature_169', 'feature_170', 'feature_171', 'feature_181',\n",
       "       'feature_185', 'feature_186', 'feature_187', 'feature_188',\n",
       "       'feature_189', 'feature_190', 'feature_191', 'feature_192',\n",
       "       'feature_193', 'feature_194', 'feature_195', 'feature_196',\n",
       "       'feature_197', 'feature_198', 'feature_199', 'feature_200',\n",
       "       'feature_201', 'feature_202', 'feature_203', 'feature_204',\n",
       "       'feature_205', 'feature_206', 'feature_207', 'feature_217',\n",
       "       'feature_221', 'feature_222', 'feature_223', 'feature_224',\n",
       "       'feature_225', 'feature_235'], dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vsave variables near zero varaince  \n",
    "#best 350\n",
    "NameNear_zer0=Near_zero.query('Nzv==True').Name.values \n",
    "# NameNear_zer0=Near_zero.query('Nzv==True & lenght>=350').Name.values \n",
    "NameNear_zer0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NameNear_zer0=NameNear_zer0.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista=['feature_15','feature_102']#,'feature_108']\n",
    "# NameNear_zer0=[x for x in NameNear_zer0 if x not in lista]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 147)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data no zero varaince\n",
    "concate_no_var=concate.drop(NameNear_zer0,axis=1)\n",
    "concate_no_var.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your selected dataframe has 147 columns.\n",
      "There are 97 columns that have missing values.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>freq_ratio</th>\n",
       "      <th>perc_unique</th>\n",
       "      <th>Nzv</th>\n",
       "      <th>lenght</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>index</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000</td>\n",
       "      <td>False</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feature_0</td>\n",
       "      <td>2.300208</td>\n",
       "      <td>0.235</td>\n",
       "      <td>False</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feature_1</td>\n",
       "      <td>1.183102</td>\n",
       "      <td>0.020</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feature_6</td>\n",
       "      <td>1.010460</td>\n",
       "      <td>7.435</td>\n",
       "      <td>False</td>\n",
       "      <td>1487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feature_7</td>\n",
       "      <td>3.000727</td>\n",
       "      <td>0.065</td>\n",
       "      <td>False</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Name  freq_ratio  perc_unique    Nzv lenght\n",
       "0      index    1.000000       50.000  False  10000\n",
       "1  feature_0    2.300208        0.235  False     47\n",
       "2  feature_1    1.183102        0.020  False      4\n",
       "3  feature_6    1.010460        7.435  False   1487\n",
       "4  feature_7    3.000727        0.065  False     13"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values_table(concate_no_var).head(10)\n",
    "Near_zero=NEAR_ZERO_VAR(concate_no_var)\n",
    "Near_zero.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best whit 7 \n",
    "#best with 5\n",
    "colonne_dummy=Near_zero.query('lenght<=5').Name.values\n",
    "colonne_dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform variables in object\n",
    "concate_no_var[colonne_dummy[:-1]] = concate_no_var[colonne_dummy[:-1]].astype('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.crosstab(index=concate_no_var['feature_6'], columns=concate_no_var['target'],dropna=False)\n",
    "#colonna 6 partilcolarmanete prolbelematica\n",
    "\n",
    "#pca per colonna 6\n",
    "# colonna_6=concate_no_var['feature_6']\n",
    "# concate_no_var=concate_no_var.drop('feature_6',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode feature_6\n",
    "import category_encoders as ce\n",
    "#1487\n",
    "encoder= ce.BaseNEncoder(cols=['feature_6'],return_df=True,base=1487)\n",
    "concate_no_var=encoder.fit_transform(concate_no_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 188)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get dmmy from missing values\n",
    "concate_no_var_dummy=pd.get_dummies(concate_no_var, dummy_na=True)\n",
    "concate_no_var_dummy.shape\n",
    "# concate_dummy=pd.get_dummies(concate.drop('feature_6',axis=1), dummy_na=True)\n",
    "concate_no_var_dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 188)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concate_nafill=concate_no_var_dummy.fillna(value=concate_no_var_dummy.mean())\n",
    "concate_nafill.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix =corr.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "concate_nafill=concate_nafill.drop(concate_nafill[to_drop], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 118)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concate_nafill.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Model Training and Results Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=concate_nafill.loc['train']\n",
    "test=concate_nafill.loc['test']\n",
    "train=train.drop('index',axis=1)\n",
    "test=test.drop('index',axis=1)\n",
    "test=test.drop('target',axis=1)\n",
    "\n",
    "X=train.drop('target',axis=1)\n",
    "y=train.target\n",
    "\n",
    "# X=pd.concat([X,s.iloc[0:10000,]],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    7017\n",
       "1.0     983\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature space holds 8000 observations and 116 features\n",
      "Unique target labels: [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Scale the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train.values), columns=X.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test.values), columns=X.columns)\n",
    "\n",
    "print(\"Feature space holds %d observations and %d features\" % X_train.shape)\n",
    "print(\"Unique target labels:\", np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from xgboost import  XGBClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validate model with Kfold stratified cross val\n",
    "kfold = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.31438127, 0.30960854, 0.27173913, 0.34170854, 0.32172471]),\n",
       " array([0.28083028, 0.32674572, 0.26466916, 0.30826141, 0.27493917])]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test differents algorithms \n",
    "random_state = 42\n",
    "classifiers = []\n",
    "# classifiers.append(SVC(random_state=random_state,C=100))\n",
    "# classifiers.append(DecisionTreeClassifier(random_state=random_state))\n",
    "# classifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.01,n_estimators=200))\n",
    "# classifiers.append(RandomForestClassifier(random_state=random_state))\n",
    "# classifiers.append(ExtraTreesClassifier(random_state=random_state))\n",
    "classifiers.append(GradientBoostingClassifier(learning_rate=0.01, max_depth= 5, max_features= 0.1, min_samples_leaf= 10, n_estimators= 300, random_state=random_state))\n",
    "# classifiers.append(VotingClassifier(estimators=[('GradientBoosting', model), ('logistic',model2)], voting='soft', n_jobs=4))\n",
    "# classifiers.append(MLPClassifier(random_state=random_state, hidden_layer_sizes=(50,50,50,50)))\n",
    "# classifiers.append(KNeighborsClassifier())\n",
    "classifiers.append(LogisticRegression(random_state = random_state))\n",
    "# classifiers.append(LinearDiscriminantAnalysis())\n",
    "# classifiers.append(XGBClassifier(random_state=random_state,max_depth=5, learning_rate=0.08, objective= 'binary:logistic'))\n",
    "\n",
    "cv_results = []\n",
    "for classifier in classifiers :\n",
    "    #imba_pipeline = make_pipeline(SMOTE(random_state=42),classifier)\n",
    "    imba_pipeline = make_pipeline(RandomOverSampler( sampling_strategy='minority',random_state=20),classifier)\n",
    "    #ros = RandomOverSampler(random_state=0)\n",
    "    #X_resampled, y_resampled = ros.fit_resample(X_train,y_train)\n",
    "    cv_results.append(cross_val_score(imba_pipeline, X_train, y = y_train, scoring = \"f1\", cv = kfold, n_jobs=-1))\n",
    "    #cv_results.append(cross_val_score(classifier,  X_train, y = y_train, scoring = \"f1\", cv = kfold, n_jobs=-1))\n",
    "\n",
    "\n",
    "cv_means = []\n",
    "cv_std = []\n",
    "for cv_result in cv_results:\n",
    "    cv_means.append(cv_result.mean())\n",
    "    cv_std.append(cv_result.std())\n",
    "\n",
    "cv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[#\"SVC\",\n",
    "#                                                                                       \"DecisionTree\",\n",
    "#                                                                                        \"AdaBoost\",\n",
    "#                                                                                         \"RandomForest\",\n",
    "#                                                                                         \"ExtraTrees\",\n",
    "                                                                                         \"GradientBoosting\",\n",
    "#                                                                                         \"MultipleLayerPerceptron\",\n",
    "#                                                                                         \"KNeighboors\",\n",
    "                                                                                       \"LogisticRegression\",\n",
    "#                                                                                         \"LinearDiscriminantAnalysis\",\n",
    "#                                                                                       \"XGBUST\"\n",
    "]})\n",
    "\n",
    "g = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\n",
    "g.set_xlabel(\"Mean f1\")\n",
    "g = g.set_title(\"Cross validation scores\")\n",
    "cv_means\n",
    "cv_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Part 4: Prediction Results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=20)\n",
    "# best randomstate 20\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(scaler.fit_transform(X_resampled.values), columns=X.columns)\n",
    "test = pd.DataFrame(scaler.transform(test.values), columns=test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=GradientBoostingClassifier(learning_rate=0.01, max_depth= 5, max_features= 0.1, min_samples_leaf= 10, n_estimators= 300, random_state=random_state)\n",
    "# model=GradientBoostingClassifier(learning_rate=0.01, max_depth= 5, max_features= 0.1, min_samples_leaf= 80, n_estimators= 400, random_state=random_state)\n",
    "# model.fit(X,y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=LogisticRegression(random_state = random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensamble\n",
    "votingC = VotingClassifier(estimators=[('GradientBoosting', model), ('logistic',model2)], voting='soft', n_jobs=4)\n",
    "votingC = votingC.fit(X,y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr = votingC.predict(test)\n",
    "probabiliti_0 = votingC.predict_proba(test)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2115.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabiliti_0[probabiliti_0>0.449]=0\n",
    "#0.449\n",
    "probabiliti_0[probabiliti_0!=0]=1\n",
    "pd.DataFrame(probabiliti_0).to_csv('m.csv',index=False,header=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
